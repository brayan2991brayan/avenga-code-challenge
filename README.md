# Books API Test Automation Framework ğŸ“š

This project is a test automation framework for the Books API, built using Python and following best practices such as Page Object Model (POM) and SOLID principles.

## ğŸ¯ Target API
- Base URL: `https://fakerestapi.azurewebsites.net`
- Endpoints: 
  - ğŸ“– `/api/v1/Books` - Books collection
  - ğŸ“• `/api/v1/Books/{id}` - Individual book

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ config/
â”‚   â””â”€â”€ dev.json           # Environment configuration
â”œâ”€â”€ logs/                  # Test execution logs
â”œâ”€â”€ reports/              # HTML test reports
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api_client.py     # API client implementation
â”‚   â”œâ”€â”€ interfaces.py     # Abstract interfaces
â”‚   â”œâ”€â”€ page_objects/     # Page Object Model implementations
â”‚   â”‚   â”œâ”€â”€ base_page.py  # Base page with common functionality
â”‚   â”‚   â””â”€â”€ books_page.py # Books-specific page object
â”‚   â””â”€â”€ utils/           # Utility modules
â”‚       â”œâ”€â”€ assertions.py # Custom test assertions
â”‚       â”œâ”€â”€ config.py     # Configuration management
â”‚       â”œâ”€â”€ logger.py     # Logging setup
â”‚       â”œâ”€â”€ retry.py      # Retry mechanism
â”‚       â””â”€â”€ test_data.py  # Test data helpers
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py       # pytest fixtures
    â””â”€â”€ test_books_api.py # API test cases
```

## âœ¨ Features

- â™»ï¸ Page Object Model design pattern
- ğŸ¯ SOLID principles implementation
- ğŸ“ Detailed HTML test reports
- ğŸ”„ Automatic retry mechanism for flaky tests
- ğŸ“Š Comprehensive logging
- âš™ï¸ Environment-based configuration
- ğŸ§ª Data-driven test approach

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9 or higher
- pip (Python package installer)
- virtualenv (recommended)

### ğŸ”§ Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd avenga-code-challenge
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸƒâ€â™‚ï¸ Running Tests

### Run all tests:
```bash
pytest --html=reports/report.html --self-contained-html
```

### Run specific test file:
```bash
pytest tests/test_books_api.py -v
```

### Run smoke tests:
```bash
pytest -m smoke -v
```

## ğŸ“Š Test Reports

After test execution, HTML reports are generated in the `reports/` directory. Open `reports/report.html` in a web browser to view detailed test results.

## ğŸ› ï¸ Development Guidelines

### Adding New Tests

1. Create page objects in `src/page_objects/` for new API endpoints
2. Implement test cases in `tests/` directory
3. Use fixtures from `conftest.py` for common setup
4. Follow existing patterns for consistency

### Best Practices

- âœ… Use type hints for better code maintainability
- âœ… Follow PEP 8 style guide
- âœ… Write descriptive docstrings
- âœ… Implement proper error handling
- âœ… Keep test cases independent
- âœ… Use appropriate markers for test categories

## ğŸ“ Documentation

- Page Objects: Each page object represents an API endpoint and encapsulates all operations and validations
- Fixtures: Common test fixtures are available in `conftest.py`
- Configuration: Environment settings in `config/dev.json`
- Logging: Test execution logs are stored in `logs/` directory

## ğŸ” Code Quality

The project maintains high code quality standards through:

- Type hints
- Comprehensive documentation
- SOLID principles
- Clean code practices
- Proper error handling
- Consistent naming conventions

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## âœ¨ Acknowledgments

- Thanks to Avenga for the code challenge opportunity
- Built with Python and pytest
- Inspired by best practices in test automation

---
Made with â¤ï¸ for Avenga Code Challenge
